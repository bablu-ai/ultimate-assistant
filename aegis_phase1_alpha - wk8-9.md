
## Week 4-5: Frontend & React UI

### Goal
Build React application with CopilotKit for agentic state management.

### Deliverables

#### 1. React Login Component
Secure login component with OAuth2 PKCE integration.

**Code Sample: Login Component**
```jsx
// File: src/components/Login.jsx

import React, { useState, useEffect } from 'react';
import { useNavigate } from 'react-router-dom';
import './Login.css';

export function Login() {
  const navigate = useNavigate();
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  // Check if returning from OAuth callback
  useEffect(() => {
    const params = new URLSearchParams(window.location.search);
    const sessionJWT = params.get('session');
    
    if (sessionJWT) {
      // Store JWT in secure sessionStorage (not localStorage!)
      sessionStorage.setItem('session_jwt', sessionJWT);
      // Redirect to dashboard
      navigate('/dashboard');
    }
  }, [navigate]);

  const handleLogin = async () => {
    setLoading(true);
    setError(null);
    
    try {
      // 1. Call backend to get OAuth2 auth URL
      const response = await fetch('/api/auth/login', {
        method: 'GET',
        credentials: 'include'  // Send cookies
      });
      
      if (!response.ok) {
        throw new Error('Failed to initiate login');
      }
      
      const data = await response.json();
      
      // 2. Redirect to OAuth2 provider
      window.location.href = data.auth_url;
      
    } catch (err) {
      setError(err.message);
      setLoading(false);
    }
  };

  return (
    <div className="login-container">
      <div className="login-card">
        <div className="logo">
          <h1>ü¶Ö Project Aegis</h1>
          <p>Regulation-Grade GenAI Banking Advisor</p>
        </div>
        
        <div className="login-form">
          <h2>Advisor Portal</h2>
          
          {error && (
            <div className="error-message" role="alert">
              ‚ö†Ô∏è {error}
            </div>
          )}
          
          <button 
            onClick={handleLogin}
            disabled={loading}
            className="login-button"
          >
            {loading ? 'Logging in...' : 'Login with Corporate Account'}
          </button>
          
          <p className="info-text">
            üí° Secure login via OAuth2 PKCE
            <br/>
            Your credentials are verified by Corporate Identity Provider
          </p>
        </div>
        
        <div className="footer">
          <p>Confidential - Internal Use Only</p>
          <p>For access issues, contact: aegis-support@example.bank.com</p>
        </div>
      </div>
    </div>
  );
}
```

#### 2. CopilotKit Integration
Integrate CopilotKit for agentic state management and real-time updates.

**Code Sample: CopilotKit Setup**
```jsx
// File: src/App.jsx

import React from 'react';
import { CopilotKit } from '@copilotkit/react-core';
import { CopilotSidebar } from '@copilotkit/react-ui';
import Dashboard from './pages/Dashboard';
import { useSession } from './hooks/useSession';

function App() {
  const { sessionJWT, user } = useSession();

  if (!sessionJWT) {
    return <Login />;
  }

  return (
    <CopilotKit
      runtimeUrl="/api/copilot"
      headers={{
        'Authorization': `Bearer ${sessionJWT}`,
        'X-Trace-ID': generateTraceId()
      }}
    >
      <div className="app-container">
        <CopilotSidebar
          defaultOpen={true}
          instructions={`
You are Aegis, a regulation-grade banking advisor assistant.

User Context:
- Name: ${user.name}
- Email: ${user.email}
- Groups: ${user.groups.join(', ')}

Your Role:
- Answer questions about banking products, regulations, and client strategies
- Always cite sources from institutional knowledge base
- Flag any policy violations to human reviewers
- Never provide advice outside your knowledge base

Safety Rules:
- If you're not sure, say "I don't know" 
- Only reference documents the user has access to
- Mark high-risk answers for human review
          `}
        />
        <main className="main-content">
          <Dashboard />
        </main>
      </div>
    </CopilotKit>
  );
}

function generateTraceId() {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    var r = Math.random() * 16 | 0, v = c === 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

export default App;
```

#### 3. Chat Interface Component
Main chat component for advisor queries.

**Code Sample: Chat Component**
```jsx
// File: src/components/ChatInterface.jsx

import React, { useState, useRef, useEffect } from 'react';
import { useCopilotContext } from '@copilotkit/react-core';
import './ChatInterface.css';

export function ChatInterface() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const messagesEndRef = useRef(null);
  const { copilot } = useCopilotContext();

  // Auto-scroll to latest message
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
  }, [messages]);

  const handleSendMessage = async (e) => {
    e.preventDefault();
    if (!input.trim()) return;

    // Add user message to UI
    const userMessage = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setLoading(true);

    try {
      // Call copilot endpoint
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${sessionStorage.getItem('session_jwt')}`,
          'X-Trace-ID': window.traceId
        },
        body: JSON.stringify({
          query: input,
          conversation_history: messages
        })
      });

      const data = await response.json();

      // Add assistant response
      const assistantMessage = {
        role: 'assistant',
        content: data.response,
        sources: data.sources,
        groundedness: data.groundedness,
        requiresApproval: data.requires_approval
      };

      setMessages(prev => [...prev, assistantMessage]);

      // If requires HITL, show approval dialog
      if (data.requires_approval) {
        showApprovalDialog(data.response, data.reason);
      }

    } catch (error) {
      console.error('Chat error:', error);
      setMessages(prev => [...prev, {
        role: 'system',
        content: '‚ùå Error processing query. Please try again.',
        isError: true
      }]);
    } finally {
      setLoading(false);
    }
  };

  const showApprovalDialog = (response, reason) => {
    // Show modal requiring advisor approval
    // This triggers HITL workflow
  };

  return (
    <div className="chat-interface">
      <div className="messages-container">
        {messages.map((msg, idx) => (
          <div key={idx} className={`message message-${msg.role}`}>
            <div className="message-content">
              {msg.content}
            </div>
            {msg.sources && (
              <div className="sources">
                <strong>Sources:</strong>
                {msg.sources.map((src, i) => (
                  <a key={i} href={src.url} target="_blank" rel="noopener noreferrer">
                    {src.title}
                  </a>
                ))}
              </div>
            )}
            {msg.groundedness !== undefined && (
              <div className={`groundedness groundedness-${msg.groundedness > 0.8 ? 'good' : 'warning'}`}>
                üìä Groundedness: {(msg.groundedness * 100).toFixed(1)}%
              </div>
            )}
          </div>
        ))}
        {loading && <div className="loading">ü§î Thinking...</div>}
        <div ref={messagesEndRef} />
      </div>

      <form onSubmit={handleSendMessage} className="input-form">
        <input
          type="text"
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="Ask about tax strategies, regulations, client scenarios..."
          disabled={loading}
          className="input-field"
        />
        <button type="submit" disabled={loading} className="send-button">
          {loading ? '‚è≥' : '‚û§'}
        </button>
      </form>
    </div>
  );
}
```

**Exit Criteria for Week 4-5**:
- ‚úÖ Login flow: OAuth2 ‚Üí Dashboard working
- ‚úÖ CopilotKit integrated with real-time updates
- ‚úÖ Chat interface rendering messages + sources
- ‚úÖ Responsive design (works on desktop + tablet)
- ‚úÖ Accessibility: WCAG 2.1 AA compliant
- ‚úÖ Performance: <2s page load time

---

## Week 6-7: Role-Based Access Control (RBAC)

### Goal
Implement document-level access control based on user groups from Active Directory.

### Deliverables

#### 1. RBAC Enforcement at Vector DB Layer

**Code Sample: RBAC-Filtered Retrieval**
```python
# File: rag/rbac_retriever.py

from typing import List, Dict
from pymilvus import Collection
from dataclasses import dataclass

@dataclass
class UserContext:
    user_id: str
    email: str
    groups: List[str]  # From AD: ["Senior Advisors", "PrivateWealth", "Compliance"]
    trace_id: str

class RBACRetriever:
    """
    Retrieves documents from Milvus Vector DB
    while enforcing RBAC based on user's AD groups
    """
    
    def __init__(self, milvus_collection: Collection):
        self.collection = milvus_collection
        
    async def retrieve_with_rbac(self,
                                 query: str,
                                 user_context: UserContext,
                                 top_k: int = 5) -> List[Dict]:
        """
        Retrieve documents matching query, filtered by user's access level
        
        Document metadata schema:
        {
            "id": "doc-123",
            "content": "...",
            "classification": "Confidential",  # Public, Internal, Confidential, Restricted
            "required_groups": ["PrivateWealth", "Senior"],  # Empty = public
            "source_url": "https://...",
            "last_updated": "2024-12-01"
        }
        """
        
        # 1. Embed query using same model as docs
        from sentence_transformers import SentenceTransformer
        encoder = SentenceTransformer('bge-m3')
        query_embedding = encoder.encode(query).tolist()
        
        # 2. Build RBAC filter expression
        # Milvus filter: "required_groups EMPTY OR required_groups IN user_groups"
        filter_expr = self._build_filter_expression(user_context.groups)
        
        # 3. Search in Milvus with filter
        search_results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param={
                "metric_type": "COSINE",
                "top_k": top_k
            },
            filter=filter_expr,
            output_fields=["id", "content", "source_url", "classification", "required_groups"]
        )
        
        # 4. Format results
        documents = []
        for hit in search_results[0]:
            doc = hit.entity
            documents.append({
                "id": doc.id,
                "content": doc.content,
                "source_url": doc.source_url,
                "classification": doc.classification,
                "relevance_score": hit.score,
                "accessible": True
            })
        
        # 5. Log to Phoenix for audit
        await self._log_retrieval(
            query=query,
            user_context=user_context,
            returned_count=len(documents),
            returned_docs=[d['id'] for d in documents]
        )
        
        return documents
    
    def _build_filter_expression(self, user_groups: List[str]) -> str:
        """
        Build Milvus filter expression for RBAC
        
        Logic:
        - Public docs: required_groups is empty
        - Internal docs: user has at least one matching group
        - Restricted docs: user listed explicitly
        """
        groups_list = "', '".join(user_groups)
        
        # "json_contains(required_groups, user_groups) OR required_groups is empty"
        filter_expr = (
            f"json_overlaps(required_groups, ['{groups_list}']) "
            f"OR json_size(required_groups) == 0"
        )
        
        return filter_expr
    
    async def _log_retrieval(self, query: str, user_context: UserContext,
                            returned_count: int, returned_docs: List[str]):
        """Log retrieval event to Phoenix for audit trail"""
        import requests
        
        log_payload = {
            "trace_id": user_context.trace_id,
            "event_type": "document_retrieval",
            "user_id": user_context.user_id,
            "user_groups": user_context.groups,
            "query": query,
            "returned_count": returned_count,
            "returned_doc_ids": returned_docs,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Send to Phoenix
        requests.post(
            "https://phoenix.example.bank.com/api/events",
            json=log_payload
        )

# Usage in FastAPI endpoint
from fastapi import Depends

async def get_user_context(request: Request) -> UserContext:
    """Dependency to extract user context from JWT"""
    token = request.headers.get("Authorization").split(" ")[1]
    payload = jwt.decode(token, JWT_SECRET)
    session = sessions_db.find_one({"_id": ObjectId(payload["session_id"])})
    
    return UserContext(
        user_id=session["user_id"],
        email=session["user_context"]["email"],
        groups=session["user_context"]["groups"],
        trace_id=request.state.trace_id
    )

@app.post("/api/retrieve")
async def retrieve_documents(query: str, 
                            user_context: UserContext = Depends(get_user_context)):
    retriever = RBACRetriever(milvus_collection)
    documents = await retriever.retrieve_with_rbac(
        query=query,
        user_context=user_context
    )
    return {"documents": documents}
```

#### 2. System Prompt with RBAC Context
Dynamically include only accessible sources in LLM system prompt.

**Code Sample: Dynamic System Prompt**
```python
# File: prompts/system_prompt_builder.py

class SystemPromptBuilder:
    """
    Builds system prompt dynamically based on user's accessible documents
    """
    
    def __init__(self, max_docs_in_context: int = 10):
        self.max_docs_in_context = max_docs_in_context
    
    def build_prompt(self, user_context: UserContext, 
                     accessible_documents: List[Dict]) -> str:
        """
        Build system prompt that:
        1. Defines Aegis role
        2. Lists accessible source documents
        3. Sets safety guardrails
        4. Specifies output format
        """
        
        docs_context = self._format_documents(accessible_documents)
        
        system_prompt = f"""
You are Aegis, a regulation-grade AI banking advisor.

## Your Role
Provide accurate, compliant financial advice to advisors by retrieving 
information from the institutional knowledge base.

## User Context
- Name: {user_context.name}
- Email: {user_context.email}
- Clearance Level: {', '.join(user_context.groups)}

## Accessible Knowledge Base
You ONLY have access to these documents:

{docs_context}

## Critical Safety Rules

1. **Always Cite Sources**: Every statement must reference a document above.
   Format: [Source: Document Name] 
   
2. **Know Your Limits**: If information is not in your knowledge base, say:
   "I don't have information about this in my knowledge base. 
    Please consult [specialist/manager/resource]."
   
3. **Never Hallucinate**: Do not generate information beyond the documents.
   Example BAD: "The 2025 tax code changes to..." (if not in documents)
   Example GOOD: "According to IRS Publication 559, the gift tax exclusion..."
   
4. **Groundedness Check**: Your responses must be grounded in the documents.
   Target: >90% of claims should directly cite source material.
   
5. **Flag Violations**: If asked about restricted topics, respond:
   "I cannot discuss this topic. Please escalate to [compliance/manager]."
   
6. **Output Format**: Use clear markdown formatting:
   - Use headers for organization
   - Use bullet points for lists
   - Always end with a "Sources" section

## Response Template

[Your answer here]

### Sources
- [Source 1: Document Name](url)
- [Source 2: Document Name](url)

## Compliance Mode
You operate under these regulations:
- SR 11-7: Model Risk Management compliance required
- GDPR: Do not expose personal data
- CCPA: Respect privacy rights
- OWASP LLM Top 10: Security controls active

---
"""
        
        return system_prompt
    
    def _format_documents(self, documents: List[Dict]) -> str:
        """Format accessible documents for inclusion in prompt"""
        if not documents:
            return "[No documents accessible to this user]"
        
        formatted = []
        for i, doc in enumerate(documents[:self.max_docs_in_context], 1):
            formatted.append(
                f"{i}. **{doc.get('title', 'Untitled')}** "
                f"(Classification: {doc.get('classification', 'Unknown')})\n"
                f"   - URL: {doc.get('source_url', 'N/A')}\n"
                f"   - Last Updated: {doc.get('last_updated', 'Unknown')}"
            )
        
        return "\n".join(formatted)

# Usage in LangGraph
builder = SystemPromptBuilder()
system_prompt = builder.build_prompt(user_context, retrieved_documents)

# Pass to LLM
response = await llm.ainvoke(
    prompt=user_query,
    system=system_prompt
)
```

**Exit Criteria for Week 6-7**:
- ‚úÖ Milvus RBAC filter expressions working
- ‚úÖ Junior advisor cannot see "Private Wealth" documents
- ‚úÖ Senior advisor can see all accessible docs
- ‚úÖ Compliance officer can audit all retrievals (Phoenix logs)
- ‚úÖ System prompt dynamically updated per user
- ‚úÖ RBAC audit trail: 100% of retrievals logged

---

## Week 8-9: PII Scrubbing & Input Guardrails

### Goal
Implement multi-layer PII detection and prompt injection defense.

### Deliverables

#### 1. Presidio PII Scrubber

**Code Sample: PII Detection & Redaction**
```python
# File: security/pii_scrubber.py

from presidio_analyzer import AnalyzerEngine
from presidio_anonymizer import AnonymizerEngine
from typing import Dict, List
import logging

class PIIScrubber:
    """
    Detects and redacts Personally Identifiable Information
    before it reaches the LLM
    """
    
    def __init__(self):
        self.analyzer = AnalyzerEngine()
        self.anonymizer = AnonymizerEngine()
        self.logger = logging.getLogger(__name__)
        
    async def scrub_input(self, text: str, trace_id: str) -> Dict:
        """
        Scan input for PII and redact
        
        Returns:
        {
            "original": "John Doe's SSN is 123-45-6789",
            "redacted": "John Doe's SSN is [SSN]",
            "detected_entities": [
                {"type": "PERSON", "text": "John Doe", "confidence": 0.95},
                {"type": "SSN", "text": "123-45-6789", "confidence": 0.99}
            ],
            "is_safe": True,
            "risk_level": "LOW"
        }
        """
        
        # 1. Analyze for PII
        entities = self.analyzer.analyze(
            text=text,
            language="en",
            entities=[
                "PERSON",           # Names
                "EMAIL_ADDRESS",    # Email addresses
                "PHONE_NUMBER",     # Phone numbers
                "SSN",             # Social Security Numbers
                "CREDIT_CARD",     # Credit card numbers
                "BANK_ACCOUNT",    # Bank account numbers
                "IBAN",            # International bank accounts
                "DATE_TIME",       # Dates (sometimes sensitive)
                "URL",             # URLs with tracking
                "IP_ADDRESS"       # IP addresses
            ]
        )
        
        # 2. Redact sensitive entities
        redacted_text = self.anonymizer.anonymize(
            text=text,
            analyzer_results=entities,
            operators={
                "SSN": {"type": "replace", "new_value": "[SSN]"},
                "CREDIT_CARD": {"type": "replace", "new_value": "[CREDIT_CARD]"},
                "PERSON": {"type": "replace", "new_value": "[PERSON]"},
                "EMAIL_ADDRESS": {"type": "replace", "new_value": "[EMAIL]"},
                "PHONE_NUMBER": {"type": "replace", "new_value": "[PHONE]"},
                "BANK_ACCOUNT": {"type": "replace", "new_value": "[ACCOUNT]"}
            }
        ).text
        
        # 3. Determine risk level
        high_risk_entities = ["SSN", "CREDIT_CARD", "BANK_ACCOUNT", "IBAN"]
        has_high_risk = any(
            e.entity_type in high_risk_entities for e in entities
        )
        
        risk_level = "CRITICAL" if has_high_risk else "LOW"
        is_safe = not has_high_risk  # Reject if high-risk PII detected
        
        # 4. Log to Phoenix for audit
        await self._log_scrubbing_event(
            trace_id=trace_id,
            original_length=len(text),
            detected_count=len(entities),
            entity_types=[e.entity_type for e in entities],
            risk_level=risk_level
        )
        
        # 5. Reject if critical PII found
        if not is_safe:
            self.logger.warning(
                f"CRITICAL PII DETECTED in trace {trace_id}: {risk_level}"
            )
            raise ValueError(
                f"Cannot process request: {risk_level} PII detected. "
                "Please remove sensitive information and try again."
            )
        
        return {
            "original": text,
            "redacted": redacted_text,
            "detected_entities": [
                {
                    "type": e.entity_type,
                    "text": e.text,
                    "confidence": e.score
                }
                for e in entities
            ],
            "is_safe": is_safe,
            "risk_level": risk_level
        }
    
    async def _log_scrubbing_event(self, trace_id: str, original_length: int,
                                   detected_count: int, entity_types: List[str],
                                   risk_level: str):
        """Log PII scrubbing event to Phoenix"""
        import requests
        
        requests.post(
            "https://phoenix.example.bank.com/api/events",
            json={
                "trace_id": trace_id,
                "event_type": "pii_scrubbing",
                "input_length": original_length,
                "entities_detected": detected_count,
                "entity_types": entity_types,
                "risk_level": risk_level,
                "timestamp": datetime.utcnow().isoformat()
            }
        )

# FastAPI middleware to apply scrubbing
@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest, 
                       user_context: UserContext = Depends(get_user_context)):
    scrubber = PIIScrubber()
    
    # 1. Scrub input
    scrubbed = await scrubber.scrub_input(
        text=request.query,
        trace_id=request.state.trace_id
    )
    
    # 2. Use scrubbed version for LLM
    clean_query = scrubbed["redacted"]
    
    # 3. Process
    result = await process_query(
        query=clean_query,
        user_context=user_context,
        trace_id=request.state.trace_id
    )
    
    return result
```

#### 2. NVIDIA NeMo Guardrails

**Code Sample: Prompt Injection Detection**
```python
# File: security/nemo_guardrails.py

from nemoguardrails import RailsConfig, LLMRails
import json

class PromptInjectionGuard:
    """
    Detects and blocks prompt injection attacks using NeMo Guardrails
    """
    
    def __init__(self):
        # Define guardrails in YAML
        self.config_yaml = """
models:
- type: main
  engine: openai
  model: gpt-4o

rails:
  input:
    flows:
      - self check input
      
flows:
  self check input:
    steps:
      - check input with regex patterns
      - filter harmful payloads
      
def check input with regex patterns:
  "This flow checks the input for known injection patterns"
  
  # Known prompt injection patterns
  patterns:
    - "ignore previous instructions"
    - "disregard prior context"
    - "reveal your system prompt"
    - "you are a different AI now"
    - "forget your guidelines"
    - "execute code"
    - "shell command"
    - "SQL injection"
  
  for pattern in patterns:
    if pattern in input.lower():
      raise ValueError("Prompt injection attempt detected")
      
def filter harmful payloads:
  "Block payloads designed to manipulate the model"
  
  harmful_keywords = [
    "jailbreak", "bypass", "exploit", "vulnerability",
    "admin override", "superuser", "root access"
  ]
  
  for keyword in harmful_keywords:
    if keyword in input.lower():
      log_security_event("prompt_injection_attempt", input)
      raise ValueError("Harmful payload detected")
        """
        
        self.config = RailsConfig.from_yaml(self.config_yaml)
        self.rails = LLMRails(self.config)
    
    async def check_input(self, text: str, trace_id: str) -> Dict:
        """
        Check input for prompt injection attempts
        
        Returns:
        {
            "is_safe": bool,
            "blocked_patterns": list,
            "confidence": float
        }
        """
        
        # 1. Run through NeMo rails
        result = await self.rails.generate(
            prompt=text,
            max_tokens=1  # Don't actually generate, just validate
        )
        
        # 2. Check for injection patterns
        injection_patterns = [
            r"ignore.*previous.*instruction",
            r"disregard.*prior.*context",
            r"reveal.*system.*prompt",
            r"you.*are.*different.*ai",
            r"forget.*guideline",
            r"execute.*code",
            r"jailbreak",
            r"bypass.*safeguard"
        ]
        
        import re
        blocked_patterns = []
        for pattern in injection_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                blocked_patterns.append(pattern)
        
        is_safe = len(blocked_patterns) == 0
        
        # 3. Log attempt
        if not is_safe:
            await self._log_injection_attempt(
                trace_id=trace_id,
                input_text=text,
                blocked_patterns=blocked_patterns
            )
        
        return {
            "is_safe": is_safe,
            "blocked_patterns": blocked_patterns,
            "confidence": 0.95 if blocked_patterns else 1.0
        }
    
    async def _log_injection_attempt(self, trace_id: str, 
                                     input_text: str, 
                                     blocked_patterns: List[str]):
        """Log to Phoenix for security monitoring"""
        import requests
        
        requests.post(
            "https://phoenix.example.bank.com/api/security-events",
            json={
                "trace_id": trace_id,
                "event_type": "prompt_injection_blocked",
                "input_text": input_text[:200],  # First 200 chars
                "matched_patterns": blocked_patterns,
                "timestamp": datetime.utcnow().isoformat(),
                "severity": "HIGH"
            }
        )

# Middleware usage
guard = PromptInjectionGuard()

@app.post("/api/chat")
async def chat_endpoint(request: ChatRequest):
    # 1. Check for prompt injection
    guard_result = await guard.check_input(
        text=request.query,
        trace_id=request.state.trace_id
    )
    
    if not guard_result["is_safe"]:
        return {
            "error": "Request contains potentially malicious patterns",
            "blocked_patterns": guard_result["blocked_patterns"]
        }
    
    # 2. Proceed with normal processing
    result = await process_query(request.query)
    return result
```

**Exit Criteria for Week 8-9**:
- ‚úÖ PII detected in 100% of test cases (SSN, CC, bank account)
- ‚úÖ Input rejection for high-risk PII: 100% block rate
- ‚úÖ Prompt injection patterns blocked: 95%+ detection rate
- ‚úÖ NeMo guardrails integrated with <100ms overhead
- ‚úÖ Phoenix logs all security events (audit trail)
- ‚úÖ Zero false positives on legitimate business text

---

## Week 10-11: LangGraph & RAG Integration

### Goal
Implement orchestration engine and RAG pipeline for knowledge retrieval.

### Deliverables - CONTINUES IN NEXT SECTION...

**Current Progress**: Weeks 1-9 Complete (85% of Phase 1)  
**Remaining**: Weeks 10-11 LangGraph + RAG + Exit Criteria

---

**PHASE 1 ALPHA DOCUMENT - SECTION 1 COMPLETE**
**Total Content: Weeks 1-9 with full code samples**